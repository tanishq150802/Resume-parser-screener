{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2yxnNrKNlnCYz2xHw5BBP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanishq150802/Resume-parser-screener/blob/main/Resume_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BVo3RsrWOFI",
        "outputId": "9dc7813e-633d-4905-a7ad-2b75cd1c04e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (40.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_file_path='/content/TanishqSelot.pdf'"
      ],
      "metadata": {
        "id": "epK_il945AdE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resume Text (for PDF)**"
      ],
      "metadata": {
        "id": "Z8e-_nWW5_C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "print(extract_text(resume_file_path))"
      ],
      "metadata": {
        "id": "x6tJSj_cWklx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpcyYsvcZQa5",
        "outputId": "48a6574d-9dc0-49e3-e8f2-e73062196c17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-QqXs5UbyVg",
        "outputId": "2e274715-09fe-4316-b7ff-39c433353359"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name** (using NLTK)"
      ],
      "metadata": {
        "id": "eanaSEYk6JRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_names(txt):\n",
        "    person_names = []\n",
        " \n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        " \n",
        "    return person_names\n",
        "\n",
        "names=extract_names(extract_text(resume_file_path))\n",
        "print(names[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsOwEuevZag_",
        "outputId": "660c4995-7dd7-46e2-a244-d02b97cff6aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanishq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name** (using StanfordNER)"
      ],
      "metadata": {
        "id": "9mAG39bDdjm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "\n",
        "PATH_TO_JAR='/content/stanford-ner.jar'\n",
        "PATH_TO_MODEL = '/content/english.all.3class.distsim.crf.ser.gz'\n",
        "\n",
        "tagger = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
        "words = nltk.word_tokenize(extract_text(resume_file_path)) \n",
        "tagged = tagger.tag(words)\n",
        "print(tagged[0][0]+' '+tagged[1][0]) #person's name is usually the first entity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_JfI79xPzpu",
        "outputId": "47922b26-5159-4e7e-9c64-fbe1955854c2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanishq Selot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name** (using SpaCy)"
      ],
      "metadata": {
        "id": "B6IfI8Shc4yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "0wBVl8eKTu05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "cmPKoizHWD7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #considering a name to be two word entity\n",
        "\n",
        "sentence = extract_text(resume_file_path)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(sentence)\n",
        "print(str(doc.ents[0]).split()[0]+' '+str(doc.ents[0]).split()[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3jbl_lcWUuC",
        "outputId": "9b252e91-792a-449a-f7eb-0badc85ae9ac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanishq Selot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phone No.**"
      ],
      "metadata": {
        "id": "E0CEAvDn6PTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "def extract_phone_number(resume_text):\n",
        "    phone = re.findall(PHONE_REG, resume_text)\n",
        " \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        return number\n",
        "    return None\n",
        "print(extract_phone_number(extract_text(resume_file_path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5234Iu1dAV2",
        "outputId": "b0eb40e0-9063-402b-827c-81a69f8428ae"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+91-7350440192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Email**"
      ],
      "metadata": {
        "id": "TCU_Bnvn6VGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        "\n",
        "def extract_emails(resume_text):\n",
        "    email = re.findall(EMAIL_REG, resume_text)\n",
        "    if(email):\n",
        "      if(len(email)==1):\n",
        "        return email[0]\n",
        "      elif(len(email)>1):\n",
        "        return email\n",
        "    return None\n",
        "print(extract_emails(extract_text(resume_file_path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ll39FQfkGx",
        "outputId": "c84e1cce-36e6-4bc8-e572-b2bfffb705dc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['me200003076@iiti.ac.in', 'tanishq1508@gmail.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skills** (using NLTK)"
      ],
      "metadata": {
        "id": "WnvPu19u6YiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_Desc = [\n",
        "    'fastapi',\n",
        "    'c++',\n",
        "    'python',\n",
        "    'html',\n",
        "    'css',\n",
        "    'node.js',\n",
        "    'database',\n",
        "    'react.js',\n",
        "    'windows',\n",
        "    'sql',\n",
        "    'php'\n",
        "]"
      ],
      "metadata": {
        "id": "nA-ZXxY_hz82"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        " \n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        " \n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        " \n",
        "    # generate n-grams (bi/tri)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        " \n",
        "    #set to keep the results in.\n",
        "    found_skills = set()\n",
        " \n",
        "    #search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in JOB_Desc:\n",
        "            found_skills.add(token)\n",
        " \n",
        "    #search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in JOB_Desc:\n",
        "            found_skills.add(ngram)\n",
        " \n",
        "    return found_skills\n",
        "\n",
        "print(extract_skills(extract_text(resume_file_path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuFGz8PMj8tQ",
        "outputId": "7641c18d-aff4-4a5a-d406-81e78658bd0e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'SQL', 'FastAPI', 'Windows', 'Python'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skills** (using stanfordNER)"
      ],
      "metadata": {
        "id": "LraBvuduhKyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_skills(input_text):\n",
        "  found=set()\n",
        "  tagger = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
        "  words = nltk.word_tokenize(input_text) \n",
        "  tagged = tagger.tag(words)\n",
        "  for t in tagged:\n",
        "    for skl in JOB_Desc:\n",
        "      if(t[0].lower()==skl):\n",
        "        found.add(t[0])\n",
        "      # if(t[0]==skl):\n",
        "      #   found.add(t[0])\n",
        "  return found\n",
        "\n",
        "print(ner_skills(extract_text(resume_file_path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guFu5yBQhcrK",
        "outputId": "7a3a439e-42e3-44b5-840c-2fed4fc86c86"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Windows', 'C++', 'FastAPI', 'SQL', 'Python', 'Node.js'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**College** (using NLTK)"
      ],
      "metadata": {
        "id": "SYTdz4A56dlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EDUCATION = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'university',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute'\n",
        "]"
      ],
      "metadata": {
        "id": "4LgiEYQnvI_N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_education(input_text):\n",
        "    organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(input_text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        "    \n",
        "    inst=[]\n",
        "    for org in organizations:\n",
        "      for ed in EDUCATION:\n",
        "        if(len(org.split())>1):\n",
        "          for i in org.split():\n",
        "            if(i.lower()==ed):\n",
        "              inst.append(org)\n",
        "              break\n",
        "    if(len(inst)>0):\n",
        "      return inst[0]\n",
        "    return None\n",
        "print(extract_education(extract_text(resume_file_path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Gxq7P7xEZA",
        "outputId": "fa1fb7bf-a17c-42d8-fd70-bdb75fb36e6c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mechanical Engineering Indian Institute Of Technology Indore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**College grade** (using stanfordNER)"
      ],
      "metadata": {
        "id": "KvHykmgHk-Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def current_grade(input_text):\n",
        "  grade={}\n",
        "  tagger = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
        "  words = nltk.word_tokenize(input_text) \n",
        "  tagged = tagger.tag(words)\n",
        "\n",
        "  def is_number(n):\n",
        "    try:\n",
        "        float(n)   # Type-casting the string to `float`.\n",
        "                   # If string is not a valid `float`, \n",
        "                   # it'll raise `ValueError` exception\n",
        "    except ValueError:\n",
        "        return False\n",
        "    return True\n",
        "  for t in tagged:\n",
        "    if(is_number(t[0])):\n",
        "      if(float(t[0])<=10.0): #cgpa is out of 10\n",
        "        grade['college']=t[0]\n",
        "        break\n",
        "  \n",
        "  return grade\n",
        "\n",
        "print(current_grade(extract_text(resume_file_path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc4aR3-TlLsA",
        "outputId": "9ba503e6-a608-4f54-d960-221bdcd7fea0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'college': '8.35'}\n"
          ]
        }
      ]
    }
  ]
}